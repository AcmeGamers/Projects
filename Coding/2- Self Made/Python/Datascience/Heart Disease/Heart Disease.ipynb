{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction ü©∫\n",
    "This dataset contains information regarding patients that have heart problems, and this dataset is used to predict whether a person has disease or not.\n",
    "\n",
    "## Basic Objective üéØ\n",
    "The basic objective of this notebook is: \n",
    "- Explore Dataset \n",
    "- Clean Dataset and performing Label Encoding for Non-Numerical Variables\n",
    "- Find numerical analysis of the dataset\n",
    "- Performing Feature Engineering\n",
    "- Splitting the Dataset into Training and Testing Datasets\n",
    "- Build Machine Learning Models to predict heart disease\n",
    "- Make Modal Comparison between different models to find the best model\n",
    "\n",
    "### Machine Learning Models ü§ñ\n",
    "We will be using seven machine learning models in this notebook:\n",
    "- K-Nearest Neighbour (KNN)\n",
    "- Random Forest\n",
    "- Decision Tree\n",
    "- Logistic Regression\n",
    "- Gaussian NB\n",
    "- Support Vector Machine\n",
    "- Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description üìã\n",
    "In this dataset, we have:\n",
    "- 9 categorical variables\n",
    "- 5 continuous variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Liberaries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions üìå\n",
    "These are the functions that are used throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def dataset_information(df):\n",
    "    print('# -------------------------\\n# Basic Dataset Information\\n# -------------------------')\n",
    "    print(\"Shape of the dataset: \", df.shape)\n",
    "    print('Number of rows: ', df.shape[0])\n",
    "    print('Number of columns: ', df.shape[1])\n",
    "\n",
    "    print('Number of Categorical Columns: ', df.select_dtypes(include=['object']).shape[1])\n",
    "    print('Number of Numerical Columns: ', df.select_dtypes(include=['int64', 'float64']).shape[1])\n",
    "    print('Missing values: ', df.isnull().sum().sum())\n",
    "\n",
    "    # ------------------------\n",
    "    # Unique Values In Dataset\n",
    "    # ------------------------\n",
    "    print('')\n",
    "    print('# ------------------------\\n# Unique Values In Dataset\\n# ------------------------')\n",
    "    print(df.nunique())\n",
    "\n",
    "\n",
    "def replace_missing_values(df):\n",
    "    df.replace('?', np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    \n",
    "def label_encoding(df, label_column):\n",
    "    print('----')\n",
    "    print('Label Column: ', label_column)\n",
    "    print(df[label_column].value_counts())\n",
    "    print('')\n",
    "    \n",
    "    # Label Encoding\n",
    "    le = LabelEncoder()\n",
    "    df[label_column] = le.fit_transform(df[label_column])\n",
    "    \n",
    "    print(df[label_column].value_counts())\n",
    "    print('')\n",
    "\n",
    "\n",
    "categorical = []\n",
    "def categorical_column_finder(df, print_data=True):\n",
    "    if print_data:\n",
    "        print('')\n",
    "        print('Categorical Columns')\n",
    "        print(df.select_dtypes(include=['object']).columns)\n",
    "        print('')\n",
    "        print('Non-Numeric Categorical Columns')\n",
    "        print(df.select_dtypes(include=['object']).columns)\n",
    "        print('')\n",
    "    \n",
    "    # emptying array\n",
    "    categorical = []\n",
    "    \n",
    "    # adding data in array\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        categorical.append(col)\n",
    "        \n",
    "    return categorical\n",
    "\n",
    "\n",
    "def label_encoding_of_categorical_columns(df, print_data = False):\n",
    "    print('')\n",
    "    data = categorical_column_finder(df, print_data)\n",
    "    for i in data:\n",
    "        label_encoding(df, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Datasets üïµÔ∏è‚Äç‚ôÄÔ∏è\n",
    "Here we will load the dataset and explore the contents present within it.\n",
    "\n",
    "Variables are assigned so any heart disease prediction model can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Dataset\n",
    "df = pd.read_csv(\"heart-1.csv\")\n",
    "df_v2 = pd.read_csv(\"heart-1.csv\")\n",
    "\n",
    "df_v3 = pd.read_csv(\"heart.csv\")\n",
    "df_v3 = df_v3.drop('target', axis=1) # Heart disease\n",
    "df_v3 = df_v3.drop('ca', axis=1)\n",
    "df_v3 = df_v3.drop('thal', axis=1)\n",
    "\n",
    "# Assigning Variables for Reuseability\n",
    "columns = ['Age','Sex','ChestPainType','RestingBP','Cholesterol','FastingBS','RestingECG','MaxHR','ExerciseAngina','Oldpeak','ST_Slope','HeartDisease'] \n",
    "\n",
    "age = df[columns[0]]\n",
    "gender = df[columns[1]]\n",
    "chest_pain_type = df[columns[2]]\n",
    "\n",
    "resting_blood_pressure = df[columns[3]]\n",
    "cholestoral = df[columns[4]]\n",
    "fasting_blood_sugar = df[columns[5]]\n",
    "\n",
    "resting_electrocardiographic_results = df[columns[6]]\n",
    "max_heart_rate_achieved = df[columns[7]]\n",
    "exercise_induced_angina = df[columns[8]]\n",
    "\n",
    "oldpeak = df[columns[9]]\n",
    "slope_of_the_peak_exercise_st_segment = df[columns[10]]\n",
    "heart_disease = df[columns[11]]\n",
    "\n",
    "\n",
    "# age = df['Age']\n",
    "# gender = df['Sex']\n",
    "# chest_pain_type = df['ChestPainType']\n",
    "\n",
    "# resting_blood_pressure = df['RestingBP']\n",
    "# cholestoral = df['Cholesterol']\n",
    "# fasting_blood_sugar = df['FastingBS']\n",
    "\n",
    "# resting_electrocardiographic_results = df['RestingECG']\n",
    "# max_heart_rate_achieved = df['MaxHR']\n",
    "# exercise_induced_angina = df['ExerciseAngina']\n",
    "\n",
    "# oldpeak = df['Oldpeak']\n",
    "# slope_of_the_peak_exercise_st_segment = df['ST_Slope']\n",
    "# heart_disease = df['HeartDisease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Information üìï\n",
    "This section will display basic information on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that will explain basic info on dataset\n",
    "dataset_information(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show data types of each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the columns that has Categorical Data\n",
    "categorical_column_finder(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding for Categorical Variables üìù\n",
    "Here we will be using label encoding for categorical variables to convert them into numerical values.\n",
    "\n",
    "An Example is, Gender has M or F as categorical variables, with the label encoder function, they will be turned into 0s and 1s for statistical calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Non-numeric Data to Numeric Values\n",
    "label_encoding_of_categorical_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying for changes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Analysis üî¢\n",
    "Now since we have examined the basic dataset, we will now look at the numerical analysis to study our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows basic statistics of the dataset non-categorical columns\n",
    "df_v2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows correlation of non-categorical columns in the dataset\n",
    "df_v2.corr()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Analysis using Heart Disease\n",
    "df.groupby('HeartDisease').size()  # target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Distribution \n",
    "df.groupby(heart_disease).mean().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulization of Dataset üìä\n",
    "Now since we have obtained the numerical analysis, we will now look at the visulization of the numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of all columns\n",
    "df.hist(figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair Plot for categorical variables with a legend to show which variables are categorical\n",
    "sns.pairplot(df_v2, hue='HeartDisease')\n",
    "print('# 0 = Blue = no heart disease')\n",
    "print('# 1 = Orange = heart disease')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for correlation between variables\n",
    "sns.heatmap(df_v2.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing üìà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing  \n",
    "X = df.drop('HeartDisease', axis=1)\n",
    "y = df['HeartDisease']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Training Samples\n",
    "# print(len(X_train))\n",
    "# print(len(y_train))\n",
    "\n",
    "# # Testing Samples\n",
    "# print(len(X_test))\n",
    "# print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbour (KNN) üìç\n",
    "The k-nearest neighbors (KNN) is a simple algorithm which is commonly used to supervise machine learning algorithm which can be used to solve both classification and regression problems. We will be using the KNN algorithm to predict the heart disease of a person based on the features and then compare the results with the other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    # ignore all caught warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    knn = KNeighborsClassifier(n_neighbors=5) \n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # NN Score\n",
    "    y_pred = knn.predict(X_test)\n",
    "    predict = knn.predict(df_v3)\n",
    "    print('Predicted values of original Dataset')\n",
    "    print(y_pred[:15])\n",
    "    print()\n",
    "    print('Predicted values of other Dataset')\n",
    "    print(predict[:15])\n",
    "    print()\n",
    "    print(\"{} NN Score: {:.2f}%\".format(2, knn.score(X_test, y_test)*100))  \n",
    "\n",
    "    # predict df_v3\n",
    "    # predict_df_v3 = knn.predict(df_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreList = []\n",
    "for i in range(1,25):\n",
    "    knn2 = KNeighborsClassifier(n_neighbors = i)\n",
    "    knn2.fit(X_train, y_train)\n",
    "    scoreList.append(knn2.score(X_test, y_test))\n",
    "    \n",
    "plt.plot(range(1,25), scoreList)\n",
    "plt.xticks(np.arange(1,25,1))\n",
    "plt.xlabel(\"K value\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()\n",
    "print(\"KNN Score Max {:.2f}%\".format((max(scoreList))*100))\n",
    "\n",
    "# Obtained Data\n",
    "KNN_data = max(scoreList)*100\n",
    "KNN_df = pd.DataFrame({'K': range(1,25), 'Score': scoreList})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest üé≤\n",
    "Random Forest is a supervised machine learning algorithm that performs both classification and regression. It is a meta-algorithm that can be used to train several decision trees at once. It is a form of ensemble learning that handles a large number of decision trees at once, while giving a higher degree of accuracy, and is computationally efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state = 1) \n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    # ignore all caught warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    y_pred = rf.predict(X_test)\n",
    "    predict = rf.predict(df_v3)\n",
    "    print('Predicted values of original Dataset')\n",
    "    print(y_pred[0:15])\n",
    "    print()\n",
    "    print('Predicted values of other Dataset')\n",
    "    print(predict[0:15])\n",
    "    print()\n",
    "\n",
    "print(\"Random Forest Algorithm Accuracy Score : {:.2f}%\".format(rf.score(X_test,y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreListRF = []\n",
    "for i in range(2,25):\n",
    "    rf2 = RandomForestClassifier(n_estimators = 1000, random_state = 1, max_leaf_nodes=i)\n",
    "    rf2.fit(X_train, y_train)\n",
    "    scoreListRF.append(rf2.score(X_test, y_test))\n",
    "    \n",
    "plt.plot(range(2,25), scoreListRF)\n",
    "plt.xticks(np.arange(2,25,1))\n",
    "plt.xlabel(\"Leaf\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()\n",
    "print(\"RF Score Max {:.2f}%\".format((max(scoreListRF))*100))\n",
    "\n",
    "# Obtained Data\n",
    "RF_data = max(scoreListRF)*100\n",
    "RF_df = pd.DataFrame({'Leaf': range(2,25), 'Score': scoreListRF})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "y_pred = rf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree üå≤\n",
    "Decision Tree are extremely useful for data analytics and machine learning commonly because they break down complex data into more manageable parts. It clearly lays out the problem so that all options can be challenged, allowing us to analyze the possible consequences of a decision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    # ignore all caught warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    y_pred = dtc.predict(X_test)\n",
    "    predict = dtc.predict(df_v3)\n",
    "    print('Predicted values of original Dataset')\n",
    "    print(y_pred[0:15])\n",
    "    print()\n",
    "    print('Predicted values of other Dataset')\n",
    "    print(predict[0:15])\n",
    "    print()\n",
    "print(\"Decision Tree Test Accuracy {:.2f}%\".format(dtc.score(X_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreListDT = []\n",
    "for i in range(2,25):\n",
    "    dtc2 = DecisionTreeClassifier(max_leaf_nodes=i)\n",
    "    dtc2.fit(X_train, y_train)\n",
    "    scoreListDT.append(dtc2.score(X_test, y_test))\n",
    "    \n",
    "plt.plot(range(2,25), scoreListDT)\n",
    "plt.xticks(np.arange(2,25,1))\n",
    "plt.xlabel(\"Leaf\")  \n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()\n",
    "print(\"DT Score Max {:.2f}%\".format((max(scoreListDT))*100))\n",
    "\n",
    "# Obtained Data\n",
    "DT_data = max(scoreListDT)*100\n",
    "DT_df= pd.DataFrame({'Leaf': range(2,25), 'Score': scoreListDT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "y_pred = dtc.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression üöÄ\n",
    "Logistic Regression is a statistical model used for classification. It is a supervised learning algorithm that works by estimating the parameters of a logistic function, which is the probability that an instance belongs to a particular class.\n",
    "\n",
    "It is commonly used in statistical software to understand the relationship between the dependent variable and one or more independent variables by estimating probabilities using a logistic regression equation. This type of analysis can help us predict the likelihood of an event happening or a choice being made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "logreg.fit(X_train, y_train) \n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    # ignore all caught warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    predict = logreg.predict(df_v3)\n",
    "    print('Predicted values of original Dataset')\n",
    "    print(y_pred[0:15])\n",
    "    print()\n",
    "    print('Predicted values of other Dataset')\n",
    "    print(predict[0:15])\n",
    "    print()\n",
    "print(\"Logistic Regression Test Accuracy {:.2f}%\".format(logreg.score(X_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreListLR = []\n",
    "for i in range(1,25):\n",
    "    logreg2 = LogisticRegression(solver='lbfgs',max_iter=1000, C=i)\n",
    "    logreg2.fit(X_train, y_train)\n",
    "    scoreListLR.append(logreg2.score(X_test, y_test))\n",
    "       \n",
    "plt.plot(range(1,25), scoreListLR)\n",
    "plt.xticks(np.arange(1,25,1))\n",
    "plt.xlabel(\"C value\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()\n",
    "print(\"LR Score Max {:.2f}%\".format((max(scoreListLR))*100))\n",
    " \n",
    "# Obtained Data\n",
    "LR_data = max(scoreListLR)*100\n",
    "LR_df = pd.DataFrame({'C value': range(1,25), 'Score': scoreListLR})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian NB üìâ\n",
    "A Gaussian Naive Bayes algorithm is a special type of NB algorithm. It's specifically used when the features have continuous values and it's also assumed that all the features are following a gaussian distribution i.e, normal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    # ignore all caught warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    y_pred = gnb.predict(X_test)\n",
    "    predict = gnb.predict(df_v3)\n",
    "    print('Predicted values of original Dataset')\n",
    "    print(y_pred[0:15])\n",
    "    print()\n",
    "    print('Predicted values of other Dataset')\n",
    "    print(predict[0:15])\n",
    "    print()\n",
    "print(\"Gaussian Naive Bayes Test Accuracy {:.2f}%\".format(gnb.score(X_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreListNB = []\n",
    "for i in range(1,25):\n",
    "    gnb2 = GaussianNB()\n",
    "    gnb2.fit(X_train, y_train)\n",
    "    scoreListNB.append(gnb2.score(X_test, y_test))\n",
    "     \n",
    "plt.plot(range(1,25), scoreListNB)\n",
    "plt.xticks(np.arange(1,25,1)) \n",
    "plt.xlabel(\"C value\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()\n",
    "print(\"NB Score Max {:.2f}%\".format((max(scoreListNB))*100))\n",
    " \n",
    "# Obtained Data\n",
    "NB_data = max(scoreListNB)*100\n",
    "NB_df = pd.DataFrame({'C value': range(1,25), 'Score': scoreListNB})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM) üìà\n",
    "Support Vector Machine (SVM) is a supervised learning algorithm that can be used to solve both classification and regression problems. It is a type of supervised learning algorithm that is used to build a model that can be used to predict the class of a new data instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    " \n",
    "with warnings.catch_warnings():\n",
    "    # ignore all caught warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    y_pred = svc.predict(X_test)\n",
    "    predict = svc.predict(df_v3)\n",
    "    print('Predicted values of original Dataset')\n",
    "    print(y_pred[0:15])\n",
    "    print()\n",
    "    print('Predicted values of other Dataset')\n",
    "    print(predict[0:15])\n",
    "    print()\n",
    "    \n",
    "print(\"SVM Test Accuracy {:.2f}%\".format(svc.score(X_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreListSVM = []\n",
    "for i in range(1,25):\n",
    "    svc2 = SVC(kernel='linear', C=i)\n",
    "    svc2.fit(X_train, y_train)\n",
    "    scoreListSVM.append(svc2.score(X_test, y_test))\n",
    "     \n",
    "plt.plot(range(1,25), scoreListSVM)\n",
    "plt.xticks(np.arange(1,25,1)) \n",
    "plt.xlabel(\"C value\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()\n",
    "print(\"SVM Score Max {:.2f}%\".format((max(scoreListSVM))*100))\n",
    " \n",
    "# Obtained Data\n",
    "SVM_data = max(scoreListSVM)*100\n",
    "SVM_df = pd.DataFrame({'C value': range(1,25), 'Score': scoreListSVM})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA) üìà\n",
    "Linear Discriminant Analysis (LDA) is a supervised learning algorithm that can be used to solve both classification and regression problems. It is a type of supervised learning algorithm that is used to build a model that can be used to predict the class of a new data instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    # ignore all caught warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    y_pred = lda.predict(X_test)\n",
    "    predict = lda.predict(df_v3)\n",
    "    print('Predicted values of original Dataset')\n",
    "    print(y_pred[0:15])\n",
    "    print()\n",
    "    print('Predicted values of other Dataset')\n",
    "    print(predict[0:15])\n",
    "    print()\n",
    "    \n",
    "print(\"LDA Test Accuracy {:.2f}%\".format(lda.score(X_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreListLDA = []\n",
    "for i in range(1,25): \n",
    "    lda2 = LinearDiscriminantAnalysis()\n",
    "    lda2.fit(X_train, y_train)\n",
    "    scoreListLDA.append(lda2.score(X_test, y_test))\n",
    "    \n",
    "plt.plot(range(1,25), scoreListLDA)\n",
    "plt.xticks(np.arange(1,25,1))\n",
    "plt.xlabel(\"C value\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()\n",
    "print(\"LDA Score Max {:.2f}%\".format((max(scoreListLDA))*100))\n",
    " \n",
    "# Obtained Data\n",
    "LDA_data = max(scoreListLDA)*100\n",
    "LDA_df = pd.DataFrame({'C value': range(1,25), 'Score': scoreListLDA})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison üî¨\n",
    "And lastly, we will compare the different models to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({'Model': [\"Logistic Regression\", \"KNN\",  \"Decision Tree\", \"Random Forest\", 'Gaussian NB', 'SVM', 'LDA'], \n",
    "                        'Accuracy': [LR_data, KNN_data, DT_data, RF_data, NB_data, SVM_data, LDA_data]})\n",
    "\n",
    "comparison.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"Logistic Regression\", \"KNN\", \"Decision Tree\", \"Random Forest\", 'Gaussian NB', 'SVM', 'LDA']\n",
    "accuracy = [LR_data, KNN_data, DT_data, RF_data, NB_data, SVM_data, LDA_data]\n",
    "colors = [\"#16bc96\", \"#1885e4\", \"#9B287B\",\"#170F11\", \"#FFCD00\", \"#FF8C00\", \"#FF0000\"]\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.yticks(np.arange(0,100,10))\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "sns.barplot(x=methods, y=accuracy, palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output üì•\n",
    "We will now write the result in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output result to csv file\n",
    "comparison.to_csv('comparison.csv', index=False)\n",
    "\n",
    "# head of the csv file\n",
    "comparison.head()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v3.to_csv('prediction.csv', index=False)\n",
    "# df_v3.head()\n",
    "\n",
    "df.to_csv('prediction-2.csv', index=False)\n",
    "# df.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9fdc8b38eeb13e23080d8401b889c3fdc15778a64593e63aa6185e4d38c6371"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
